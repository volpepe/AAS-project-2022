{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.EnumValueDescriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _DATATYPE = _descriptor.EnumDescriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _TENSORPROTO = _descriptor.Descriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _ATTRVALUE_LISTVALUE = _descriptor.Descriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  if hasattr(pil_image, 'HAMMING'):\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  if hasattr(pil_image, 'BOX'):\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  if hasattr(pil_image, 'LANCZOS'):\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "from agent import Agent\n",
    "from state import State\n",
    "from action import Action\n",
    "from variables import GAMMA, SIGMA\n",
    "\n",
    "class BaselineActorCriticAgent(Agent):\n",
    "    def __init__(self, num_actions, optimizer, discount:float=GAMMA) -> None:\n",
    "        super().__init__(num_actions, optimizer)\n",
    "        self.num_actions = num_actions\n",
    "        self.optimizer = optimizer\n",
    "        self.discount = discount\n",
    "        self.dense1 = layers.Dense(64)\n",
    "        self.dense2 = layers.Dense(64)\n",
    "        self.actor  = layers.Dense(self.num_actions)        # Produce logits\n",
    "        self.critic = layers.Dense(1)                      # Produce the state-value directly\n",
    "\n",
    "    def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        x = self.dense2(self.dense1(inputs))\n",
    "        # Then we produce the policy values\n",
    "        action_logits = self.actor(x)                    # 1xnum_actions\n",
    "        action_probs  = tf.nn.softmax(action_logits)     # 1xnum_actions probabilities\n",
    "        # Avoid producing a tensor containing probability 0 for some actions.\n",
    "        action_probs = tf.clip_by_value(action_probs, 1e-10, 1.0)\n",
    "        # ... and the state value.\n",
    "        state_value = self.critic(x)                    # 1x1\n",
    "        return action_logits, action_probs, state_value\n",
    "\n",
    "    def choose_action(self, state:np.ndarray) -> Tuple:\n",
    "        action_logits, action_probs, state_value = self(\n",
    "            tf.expand_dims(tf.cast(state, tf.float32), axis=0))\n",
    "        # Sample from the actions probability distribution\n",
    "        action = tf.random.categorical(action_logits, 1)\n",
    "        return {\n",
    "            'action': action.numpy()[0,0],\n",
    "            'policy': action_probs[0],\n",
    "            'value' : state_value[0]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineActorCriticAgent(2, Adam(learning_rate=1e-2, clipnorm=40.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, state):\n",
    "    a_info = model.choose_action(state)\n",
    "    next_state, reward, done, _ = env.step(a_info['action'])\n",
    "    return (state, a_info, reward, next_state, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(env, init_state):\n",
    "    ep_steps = 0\n",
    "    episode_buffer = deque([], maxlen=200)\n",
    "    state = init_state\n",
    "    for step in tf.range(200):\n",
    "        ep_steps += 1\n",
    "        state, a_info, reward, next_state, done = play_one_step(env, state)\n",
    "        episode_buffer.append((state, a_info, reward, next_state, done))\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    states, actions, rewards, next_states, dones = (\n",
    "        np.array([experience[i] for experience in list(episode_buffer)]) \n",
    "        for i in range(5))\n",
    "    # Compute returns\n",
    "    returns = []\n",
    "    discounted_sum = tf.constant(0.0)\n",
    "    for i in tf.range(ep_steps-1, -1, -1):\n",
    "        discounted_sum = rewards[i] + 0.99 * discounted_sum\n",
    "        returns.append(discounted_sum)\n",
    "    returns = np.stack(returns[::-1])\n",
    "    returns = (returns - tf.math.reduce_mean(returns)) / (tf.math.reduce_std(returns))\n",
    "    return states, actions, rewards, returns, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(env, initial_state):\n",
    "    # We open a GradientTape because we want to reverse these operations to obtain the gradient\n",
    "    # of the loss with respect to the model's parameter\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 1) Get the experience playing the episode\n",
    "        states, actions, rewards, returns, next_states, dones = play_episode(env, initial_state)\n",
    "        v_st_pred = tf.squeeze(tf.stack([a['value'] for a in actions]))\n",
    "        a_probs = tf.stack([a['policy'] for a in actions])\n",
    "        a_indices = tf.stack([a['action'] for a in actions])\n",
    "        a_probs = tf.gather(a_probs, a_indices, batch_dims=1)\n",
    "        a_log_probs = tf.math.log(a_probs)\n",
    "        # Compute delta\n",
    "        delta = returns - v_st_pred\n",
    "        # Actor loss\n",
    "        actor_loss = -tf.reduce_sum(tf.expand_dims(delta, axis=-1)*a_log_probs)\n",
    "        # Critic loss\n",
    "        critic_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)(v_st_pred, returns)\n",
    "        # Entropy loss\n",
    "        # entropy_loss = -tf.reduce_sum(a_log_probs*a_probs)\n",
    "        # Total loss\n",
    "        loss = tf.reduce_sum(actor_loss + critic_loss) #+ 0.01*entropy_loss)\n",
    "    # 9) Obtain the gradient of the loss with respect to the model's parameters\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    # 10) Apply the update\n",
    "    model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode reward: 17.00, mean reward: 20.43: 100%|██████████| 800/800 [01:35<00:00,  8.41it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "rewards_mean = 0\n",
    "with trange(800) as t:\n",
    "    for ep_num in t:\n",
    "        initial_state = env.reset()\n",
    "        rewards = training_step(env, initial_state)\n",
    "        episode_reward = sum(rewards)\n",
    "        rewards_mean = rewards_mean + (episode_reward-rewards_mean)/(ep_num+1)\n",
    "        t.set_description(f'Episode reward: {episode_reward:.2f}, mean reward: {rewards_mean:.2f}')\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With curiosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineActorCriticAgent(2, Adam(learning_rate=1e-2, clipnorm=40.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import losses, Model\n",
    "from keras.layers import Layer\n",
    "from variables import *\n",
    "\n",
    "class ICM(Model):\n",
    "    def __init__(self, num_actions, optimizer, beta=BETA, eta=ETA, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_actions = num_actions\n",
    "        self.optimizer = optimizer\n",
    "        self.beta = beta                                # Weight of the forward model loss against the inverse model loss\n",
    "        self.eta = eta                                  # Scaling factor for the intrinsic reward signal\n",
    "        self.encoding_layer = EncodingLayer()\n",
    "        self.forward_model  = ForwardModel(num_actions)\n",
    "        self.inverse_model  = InverseModel(num_actions)\n",
    "\n",
    "    def change_scaling_factor(self, new_eta) -> None:\n",
    "        '''\n",
    "        Function that allows dynamic changes to the scaling factor.\n",
    "        '''\n",
    "        self.eta = new_eta\n",
    "\n",
    "    def call(self, inputs, training=False) -> tf.Tensor:\n",
    "        # Inputs are the state St, action At and state St+1\n",
    "        # States are [1,42,42,4] tensors, while action At is a [1,num_actions] tensor\n",
    "        st, at, st1 = inputs\n",
    "        # Computing state encodings\n",
    "        e_st, e_st1 = self.encoding_layer((st, st1))\n",
    "        # Predict the encoding of state st1 and the action.\n",
    "        pred_e_st1 = self.forward_model((at, e_st))\n",
    "        pred_at = self.inverse_model((e_st, e_st1))\n",
    "        if training:\n",
    "            # We compute the loss of the ICM. It's a composite loss, because we have two \n",
    "            # communicating modules:\n",
    "            # - The loss of the forward model is a regression loss between the \n",
    "            #   ground truth encoding and the predicted one\n",
    "            # - The loss of the inverse model is a cross-entropy loss between the\n",
    "            #   ground truth action probability distribution and the predicted one.\n",
    "            loss_inverse = losses.categorical_crossentropy(at, pred_at)\n",
    "            loss_forward = losses.huber(e_st1, pred_e_st1, delta=1.0)\n",
    "            loss_value = (1-self.beta)*tf.reduce_sum(loss_inverse) + self.beta*tf.reduce_sum(loss_forward)\n",
    "            # Use the add_loss API to retrieve this value as a loss to minimize later\n",
    "            self.add_loss(ICM_LW*loss_value)\n",
    "        # Finally, compute the output (intrinsic reward)\n",
    "        # ri = self.eta/2*tf.norm(pred_e_st1 - e_st1)\n",
    "        ri = tf.math.minimum(CLIP_RE, self.eta/2*tf.norm(pred_e_st1 - e_st1))\n",
    "        return ri\n",
    "\n",
    "\n",
    "class EncodingLayer(Layer):\n",
    "    '''\n",
    "    Utility layer for computing the encodings of the states, separated from the rest since \n",
    "    encodings are shared between the inverse and forward models.\n",
    "    '''\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.dense   = layers.Dense(288)\n",
    "        self.dropout = layers.Dropout(0.2)\n",
    "        self.flatten = layers.Flatten()\n",
    "    \n",
    "    def call(self, inputs) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        # Inputs are the states St and St+1, [1, 42, 42, 4] tensors\n",
    "        st, st1 = inputs\n",
    "        # Compute encoding of state st and st1\n",
    "        # 1x288 <- 1x3x3x32 <- 1x6x6x32 <- 1x11x11x32 <- 1x21x21x32 <- 1x42x42x4\n",
    "        e_st  = self.dense(st)\n",
    "        e_st1 = self.dense(st1)\n",
    "        return e_st, e_st1\n",
    "\n",
    "\n",
    "class ForwardModel(Layer):\n",
    "    '''\n",
    "    The forward model of the ICM takes as input the action At (one-hot encoded)\n",
    "    and the encoding of the state St (`e(St)`). It tries to predict the encoding\n",
    "    of state St+1 (`pred[e(St+1)]`)\n",
    "    '''\n",
    "    def __init__(self, num_actions, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_actions = num_actions\n",
    "        self.concat = layers.Concatenate(axis=1)\n",
    "        self.dense1 = layers.Dense(128, activation='relu')                                                           # Original is 256\n",
    "        self.dense2 = layers.Dense(288)\n",
    "\n",
    "    def call(self, inputs) -> tf.Tensor:\n",
    "        # Inputs: the action At and the encoding of the state e(St)\n",
    "        at, e_st = inputs\n",
    "        # at is [1, num_actions]\n",
    "        # enc_st is [1, 288]\n",
    "        x = self.concat([at, e_st])                 # [1, num_actions + 288]\n",
    "        pred_e_st1 = self.dense2(self.dense1(x))    # [1, 288]\n",
    "        return pred_e_st1\n",
    "\n",
    "\n",
    "class InverseModel(Layer):\n",
    "    '''\n",
    "    The inverse model of the ICM takes as input the encoding of states St and St+1\n",
    "    `e(St)` and `e(St+1)` and tries to predict the action (`pred[At]`).\n",
    "    '''\n",
    "    def __init__(self, num_actions, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_actions = num_actions\n",
    "        self.concat  = layers.Concatenate(axis=1)\n",
    "        self.dense1  = layers.Dense(128, activation='relu')                                                          # Original is 256\n",
    "        self.dense2  = layers.Dense(self.num_actions, activation='softmax')\n",
    "\n",
    "    def call(self, inputs) -> tf.Tensor:\n",
    "        e_st, e_st1 = inputs\n",
    "        # Concatenate the encodings\n",
    "        e_states = self.concat([e_st, e_st1])             # [1, 288*2]\n",
    "        # Dense layers for action prediction\n",
    "        pred_at = self.dense2(self.dense1(e_states))      # [1, num_actions], probability distribution\n",
    "        return pred_at\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "curiosity_model = ICM(2, optimizer=keras.optimizers.Adam(learning_rate=1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "intrinsic_rewards = []\n",
    "\n",
    "def play_one_step_with_curiosity(env, state):\n",
    "    a_info = model.choose_action(state)\n",
    "    next_state, reward, done, _ = env.step(a_info['action'])\n",
    "    intrinsic_reward = curiosity_model(\n",
    "        (tf.cast(tf.expand_dims(state, axis=0     ), tf.float32), \n",
    "         tf.cast(tf.expand_dims(tf.one_hot(a_info['action'], depth=2), axis=0), tf.float32), \n",
    "         tf.cast(tf.expand_dims(next_state, axis=0), tf.float32)),\n",
    "         training=True\n",
    "    )\n",
    "    reward += intrinsic_reward\n",
    "    intrinsic_rewards.append(intrinsic_reward)\n",
    "    return (state, a_info, reward, next_state, done)\n",
    "\n",
    "def play_episode_with_curiosity(env, init_state):\n",
    "    ep_steps = 0\n",
    "    episode_buffer = deque([], maxlen=200)\n",
    "    state = init_state\n",
    "    for step in tf.range(200):\n",
    "        ep_steps += 1\n",
    "        state, a_info, reward, next_state, done = play_one_step_with_curiosity(env, state)\n",
    "        episode_buffer.append((state, a_info, reward, next_state, done))\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    states, actions, rewards, next_states, dones = (\n",
    "        np.array([experience[i] for experience in list(episode_buffer)]) \n",
    "        for i in range(5))\n",
    "    # Compute returns\n",
    "    returns = []\n",
    "    discounted_sum = tf.constant(0.0)\n",
    "    for i in tf.range(ep_steps-1, -1, -1):\n",
    "        discounted_sum = rewards[i] + 0.99 * discounted_sum\n",
    "        returns.append(discounted_sum)\n",
    "    returns = np.stack(returns[::-1])\n",
    "    returns = (returns - tf.math.reduce_mean(returns)) / (tf.math.reduce_std(returns))\n",
    "    return states, actions, rewards, returns, next_states, dones\n",
    "\n",
    "def training_step_with_curiosity(env, initial_state):\n",
    "    # We open a GradientTape because we want to reverse these operations to obtain the gradient\n",
    "    # of the loss with respect to the model's parameter\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # 1) Get the experience playing the episode\n",
    "        states, actions, rewards, returns, next_states, dones = play_episode_with_curiosity(env, initial_state)\n",
    "        v_st_pred = tf.squeeze(tf.stack([a['value'] for a in actions]))\n",
    "        a_probs = tf.stack([a['policy'] for a in actions])\n",
    "        a_indices = tf.stack([a['action'] for a in actions])\n",
    "        a_probs = tf.gather(a_probs, a_indices, batch_dims=1)\n",
    "        a_log_probs = tf.math.log(a_probs)\n",
    "        # Compute delta\n",
    "        delta = returns - v_st_pred\n",
    "        # Actor loss\n",
    "        actor_loss = -tf.reduce_sum(tf.expand_dims(delta, axis=-1)*a_log_probs)\n",
    "        # Critic loss\n",
    "        critic_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)(v_st_pred, returns)\n",
    "        # Entropy loss\n",
    "        # entropy_loss = -tf.reduce_sum(a_log_probs*a_probs)\n",
    "        # Total loss\n",
    "        loss = tf.reduce_sum(actor_loss + critic_loss) #+ 0.01*entropy_loss)\n",
    "        # Get curiosity loss\n",
    "        intrinsic_loss = tf.reduce_sum(curiosity_model.losses)\n",
    "        total_loss = loss + intrinsic_loss\n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    grads_curiosity = tape.gradient(total_loss, curiosity_model.trainable_variables)\n",
    "    curiosity_model.optimizer.apply_gradients(zip(grads_curiosity, curiosity_model.trainable_variables))\n",
    "    del tape\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode reward: 9.41, mean reward: 9.73, intrinsic: 0.41:  58%|█████▊    | 462/800 [01:02<00:45,  7.40it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mf:\\Dump\\Coding\\Progetto Autonomous\\AAS-project-2022\\src\\test_algorithms.ipynb Cella 13\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m ep_num \u001b[39min\u001b[39;00m t:\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=5'>6</a>\u001b[0m     initial_state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=6'>7</a>\u001b[0m     rewards \u001b[39m=\u001b[39m training_step_with_curiosity(env, initial_state)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=7'>8</a>\u001b[0m     episode_reward \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(rewards)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=8'>9</a>\u001b[0m     intrinsic_reward \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(intrinsic_rewards)\n",
      "\u001b[1;32mf:\\Dump\\Coding\\Progetto Autonomous\\AAS-project-2022\\src\\test_algorithms.ipynb Cella 13\u001b[0m in \u001b[0;36mtraining_step_with_curiosity\u001b[1;34m(env, initial_state)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=39'>40</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining_step_with_curiosity\u001b[39m(env, initial_state):\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=40'>41</a>\u001b[0m     \u001b[39m# We open a GradientTape because we want to reverse these operations to obtain the gradient\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=41'>42</a>\u001b[0m     \u001b[39m# of the loss with respect to the model's parameter\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=42'>43</a>\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape(persistent\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m tape:\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=43'>44</a>\u001b[0m         \u001b[39m# 1) Get the experience playing the episode\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=44'>45</a>\u001b[0m         states, actions, rewards, returns, next_states, dones \u001b[39m=\u001b[39m play_episode_with_curiosity(env, initial_state)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=45'>46</a>\u001b[0m         v_st_pred \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msqueeze(tf\u001b[39m.\u001b[39mstack([a[\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m actions]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=46'>47</a>\u001b[0m         a_probs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mstack([a[\u001b[39m'\u001b[39m\u001b[39mpolicy\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m actions])\n",
      "\u001b[1;32mf:\\Dump\\Coding\\Progetto Autonomous\\AAS-project-2022\\src\\test_algorithms.ipynb Cella 13\u001b[0m in \u001b[0;36mplay_episode_with_curiosity\u001b[1;34m(env, init_state)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mrange(\u001b[39m200\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=20'>21</a>\u001b[0m     ep_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=21'>22</a>\u001b[0m     state, a_info, reward, next_state, done \u001b[39m=\u001b[39m play_one_step_with_curiosity(env, state)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=22'>23</a>\u001b[0m     episode_buffer\u001b[39m.\u001b[39mappend((state, a_info, reward, next_state, done))\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=23'>24</a>\u001b[0m     \u001b[39mif\u001b[39;00m done:\n",
      "\u001b[1;32mf:\\Dump\\Coding\\Progetto Autonomous\\AAS-project-2022\\src\\test_algorithms.ipynb Cella 13\u001b[0m in \u001b[0;36mplay_one_step_with_curiosity\u001b[1;34m(env, state)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=3'>4</a>\u001b[0m a_info \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mchoose_action(state)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=4'>5</a>\u001b[0m next_state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(a_info[\u001b[39m'\u001b[39m\u001b[39maction\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=5'>6</a>\u001b[0m intrinsic_reward \u001b[39m=\u001b[39m curiosity_model(\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=6'>7</a>\u001b[0m     (tf\u001b[39m.\u001b[39;49mcast(tf\u001b[39m.\u001b[39;49mexpand_dims(state, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m     ), tf\u001b[39m.\u001b[39;49mfloat32), \n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=7'>8</a>\u001b[0m      tf\u001b[39m.\u001b[39;49mcast(tf\u001b[39m.\u001b[39;49mexpand_dims(tf\u001b[39m.\u001b[39;49mone_hot(a_info[\u001b[39m'\u001b[39;49m\u001b[39maction\u001b[39;49m\u001b[39m'\u001b[39;49m], depth\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m), axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m), tf\u001b[39m.\u001b[39;49mfloat32), \n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=8'>9</a>\u001b[0m      tf\u001b[39m.\u001b[39;49mcast(tf\u001b[39m.\u001b[39;49mexpand_dims(next_state, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m), tf\u001b[39m.\u001b[39;49mfloat32)),\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=9'>10</a>\u001b[0m      training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=10'>11</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=11'>12</a>\u001b[0m reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m intrinsic_reward\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=12'>13</a>\u001b[0m intrinsic_rewards\u001b[39m.\u001b[39mappend(intrinsic_reward)\n",
      "File \u001b[1;32mc:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\keras\\engine\\base_layer.py:1096\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> 1096\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1098\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1099\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\keras\\utils\\traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 92\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     93\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     94\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     95\u001b[0m     \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "\u001b[1;32mf:\\Dump\\Coding\\Progetto Autonomous\\AAS-project-2022\\src\\test_algorithms.ipynb Cella 13\u001b[0m in \u001b[0;36mICM.call\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=29'>30</a>\u001b[0m pred_at \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minverse_model((e_st, e_st1))\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=30'>31</a>\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=31'>32</a>\u001b[0m     \u001b[39m# We compute the loss of the ICM. It's a composite loss, because we have two \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=32'>33</a>\u001b[0m     \u001b[39m# communicating modules:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=35'>36</a>\u001b[0m     \u001b[39m# - The loss of the inverse model is a cross-entropy loss between the\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=36'>37</a>\u001b[0m     \u001b[39m#   ground truth action probability distribution and the predicted one.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=37'>38</a>\u001b[0m     loss_inverse \u001b[39m=\u001b[39m losses\u001b[39m.\u001b[39;49mcategorical_crossentropy(at, pred_at)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=38'>39</a>\u001b[0m     loss_forward \u001b[39m=\u001b[39m losses\u001b[39m.\u001b[39mhuber(e_st1, pred_e_st1, delta\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Dump/Coding/Progetto%20Autonomous/AAS-project-2022/src/test_algorithms.ipynb#ch0000011?line=39'>40</a>\u001b[0m     loss_value \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta)\u001b[39m*\u001b[39mtf\u001b[39m.\u001b[39mreduce_sum(loss_inverse) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta\u001b[39m*\u001b[39mtf\u001b[39m.\u001b[39mreduce_sum(loss_forward)\n",
      "File \u001b[1;32mc:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1083\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\keras\\losses.py:1789\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[1;34m(y_true, y_pred, from_logits, label_smoothing, axis)\u001b[0m\n\u001b[0;32m   1784\u001b[0m   \u001b[39mreturn\u001b[39;00m y_true \u001b[39m*\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m label_smoothing) \u001b[39m+\u001b[39m (label_smoothing \u001b[39m/\u001b[39m num_classes)\n\u001b[0;32m   1786\u001b[0m y_true \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39msmart_cond\u001b[39m.\u001b[39msmart_cond(label_smoothing, _smooth_labels,\n\u001b[0;32m   1787\u001b[0m                                \u001b[39mlambda\u001b[39;00m: y_true)\n\u001b[1;32m-> 1789\u001b[0m \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39;49mcategorical_crossentropy(\n\u001b[0;32m   1790\u001b[0m     y_true, y_pred, from_logits\u001b[39m=\u001b[39;49mfrom_logits, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32mc:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1083\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\keras\\backend.py:5098\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[1;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[0;32m   5095\u001b[0m   from_logits \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   5097\u001b[0m \u001b[39mif\u001b[39;00m from_logits:\n\u001b[1;32m-> 5098\u001b[0m   \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49msoftmax_cross_entropy_with_logits(\n\u001b[0;32m   5099\u001b[0m       labels\u001b[39m=\u001b[39;49mtarget, logits\u001b[39m=\u001b[39;49moutput, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   5101\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(output, (tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mEagerTensor, tf\u001b[39m.\u001b[39mVariable)) \u001b[39mand\u001b[39;00m\n\u001b[0;32m   5102\u001b[0m     output\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSoftmax\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(output, \u001b[39m'\u001b[39m\u001b[39m_keras_history\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m   5103\u001b[0m   \u001b[39m# When softmax activation function is used for output operation, we\u001b[39;00m\n\u001b[0;32m   5104\u001b[0m   \u001b[39m# use logits from the softmax function directly to compute loss in order\u001b[39;00m\n\u001b[0;32m   5105\u001b[0m   \u001b[39m# to prevent collapsing zero when training.\u001b[39;00m\n\u001b[0;32m   5106\u001b[0m   \u001b[39m# See b/117284466\u001b[39;00m\n\u001b[0;32m   5107\u001b[0m   \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(output\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39minputs) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1083\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:4008\u001b[0m, in \u001b[0;36msoftmax_cross_entropy_with_logits_v2\u001b[1;34m(labels, logits, axis, name)\u001b[0m\n\u001b[0;32m   3949\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mnn.softmax_cross_entropy_with_logits\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[0;32m   3950\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[0;32m   3951\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msoftmax_cross_entropy_with_logits_v2\u001b[39m(labels, logits, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   3952\u001b[0m   \u001b[39m\"\"\"Computes softmax cross entropy between `logits` and `labels`.\u001b[39;00m\n\u001b[0;32m   3953\u001b[0m \n\u001b[0;32m   3954\u001b[0m \u001b[39m  Measures the probability error in discrete classification tasks in which the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4006\u001b[0m \u001b[39m    not have the last dimension of `labels`.\u001b[39;00m\n\u001b[0;32m   4007\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4008\u001b[0m   \u001b[39mreturn\u001b[39;00m softmax_cross_entropy_with_logits_v2_helper(\n\u001b[0;32m   4009\u001b[0m       labels\u001b[39m=\u001b[39;49mlabels, logits\u001b[39m=\u001b[39;49mlogits, axis\u001b[39m=\u001b[39;49maxis, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1083\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:548\u001b[0m, in \u001b[0;36mdeprecated_args.<locals>.deprecated_wrapper.<locals>.new_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    540\u001b[0m         _PRINTED_WARNING[(func, arg_name)] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    541\u001b[0m       logging\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    542\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mFrom \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: calling \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m (from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) with \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is deprecated and will \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    543\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mbe removed \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mInstructions for updating:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    546\u001b[0m           \u001b[39m'\u001b[39m\u001b[39min a future version\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m date \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mafter \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m date),\n\u001b[0;32m    547\u001b[0m           instructions)\n\u001b[1;32m--> 548\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:4104\u001b[0m, in \u001b[0;36msoftmax_cross_entropy_with_logits_v2_helper\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   4101\u001b[0m input_shape \u001b[39m=\u001b[39m array_ops\u001b[39m.\u001b[39mshape(precise_logits)\n\u001b[0;32m   4103\u001b[0m \u001b[39m# Make precise_logits and labels into matrices.\u001b[39;00m\n\u001b[1;32m-> 4104\u001b[0m precise_logits \u001b[39m=\u001b[39m _flatten_outer_dims(precise_logits)\n\u001b[0;32m   4105\u001b[0m labels \u001b[39m=\u001b[39m _flatten_outer_dims(labels)\n\u001b[0;32m   4107\u001b[0m \u001b[39m# Do the actual op computation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:3715\u001b[0m, in \u001b[0;36m_flatten_outer_dims\u001b[1;34m(logits)\u001b[0m\n\u001b[0;32m   3713\u001b[0m \u001b[39m\"\"\"Flattens logits' outer dimensions and keep its last dimension.\"\"\"\u001b[39;00m\n\u001b[0;32m   3714\u001b[0m rank \u001b[39m=\u001b[39m array_ops\u001b[39m.\u001b[39mrank(logits)\n\u001b[1;32m-> 3715\u001b[0m last_dim_size \u001b[39m=\u001b[39m array_ops\u001b[39m.\u001b[39;49mslice(\n\u001b[0;32m   3716\u001b[0m     array_ops\u001b[39m.\u001b[39;49mshape(logits), [math_ops\u001b[39m.\u001b[39;49msubtract(rank, \u001b[39m1\u001b[39;49m)], [\u001b[39m1\u001b[39;49m])\n\u001b[0;32m   3717\u001b[0m output \u001b[39m=\u001b[39m array_ops\u001b[39m.\u001b[39mreshape(logits, array_ops\u001b[39m.\u001b[39mconcat([[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], last_dim_size], \u001b[39m0\u001b[39m))\n\u001b[0;32m   3719\u001b[0m \u001b[39m# Set output shape if known.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1083\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1119\u001b[0m, in \u001b[0;36mslice\u001b[1;34m(input_, begin, size, name)\u001b[0m\n\u001b[0;32m   1067\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mslice\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1068\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[0;32m   1069\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mslice\u001b[39m(input_, begin, size, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1070\u001b[0m   \u001b[39m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m   \u001b[39m\"\"\"Extracts a slice from a tensor.\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \n\u001b[0;32m   1073\u001b[0m \u001b[39m  See also `tf.strided_slice`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1117\u001b[0m \u001b[39m    A `Tensor` the same type as `input_`.\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1119\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_array_ops\u001b[39m.\u001b[39;49m_slice(input_, begin, size, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:9576\u001b[0m, in \u001b[0;36m_slice\u001b[1;34m(input, begin, size, name)\u001b[0m\n\u001b[0;32m   9574\u001b[0m   \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   9575\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 9576\u001b[0m   \u001b[39mreturn\u001b[39;00m _slice_eager_fallback(\n\u001b[0;32m   9577\u001b[0m       \u001b[39minput\u001b[39;49m, begin, size, name\u001b[39m=\u001b[39;49mname, ctx\u001b[39m=\u001b[39;49m_ctx)\n\u001b[0;32m   9578\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_SymbolicException:\n\u001b[0;32m   9579\u001b[0m   \u001b[39mpass\u001b[39;00m  \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:9602\u001b[0m, in \u001b[0;36m_slice_eager_fallback\u001b[1;34m(input, begin, size, name, ctx)\u001b[0m\n\u001b[0;32m   9600\u001b[0m _inputs_flat \u001b[39m=\u001b[39m [\u001b[39minput\u001b[39m, begin, size]\n\u001b[0;32m   9601\u001b[0m _attrs \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mT\u001b[39m\u001b[39m\"\u001b[39m, _attr_T, \u001b[39m\"\u001b[39m\u001b[39mIndex\u001b[39m\u001b[39m\"\u001b[39m, _attr_Index)\n\u001b[1;32m-> 9602\u001b[0m _result \u001b[39m=\u001b[39m _execute\u001b[39m.\u001b[39;49mexecute(\u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mSlice\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m1\u001b[39;49m, inputs\u001b[39m=\u001b[39;49m_inputs_flat, attrs\u001b[39m=\u001b[39;49m_attrs,\n\u001b[0;32m   9603\u001b[0m                            ctx\u001b[39m=\u001b[39;49mctx, name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m   9604\u001b[0m \u001b[39mif\u001b[39;00m _execute\u001b[39m.\u001b[39mmust_record_gradient():\n\u001b[0;32m   9605\u001b[0m   _execute\u001b[39m.\u001b[39mrecord_gradient(\n\u001b[0;32m   9606\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mSlice\u001b[39m\u001b[39m\"\u001b[39m, _inputs_flat, _attrs, _result)\n",
      "File \u001b[1;32mc:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "rewards_mean = 0\n",
    "with trange(800) as t:\n",
    "    for ep_num in t:\n",
    "        initial_state = env.reset()\n",
    "        rewards = training_step_with_curiosity(env, initial_state)\n",
    "        episode_reward = sum(rewards)\n",
    "        intrinsic_reward = sum(intrinsic_rewards)\n",
    "        intrinsic_rewards.clear()\n",
    "        rewards_mean = rewards_mean + (episode_reward-rewards_mean)/(ep_num+1)\n",
    "        t.set_description(f'Episode reward: {episode_reward:.2f}, mean reward: {rewards_mean:.2f}, intrinsic: {intrinsic_reward:.2f}')\n",
    "        \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab22f16c00685cd6fecce7c07f73b88874846b6bd1a47397f6c275263024f85c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
