{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.EnumValueDescriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _DATATYPE = _descriptor.EnumDescriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _TENSORPROTO = _descriptor.Descriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _ATTRVALUE_LISTVALUE = _descriptor.Descriptor(\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  if hasattr(pil_image, 'HAMMING'):\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  if hasattr(pil_image, 'BOX'):\n",
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  if hasattr(pil_image, 'LANCZOS'):\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "from agent import Agent\n",
    "from state import State\n",
    "from action import Action\n",
    "from variables import GAMMA, SIGMA\n",
    "\n",
    "class BaselineActorCriticAgent(Agent):\n",
    "    def __init__(self, num_actions, optimizer, discount:float=GAMMA) -> None:\n",
    "        super().__init__(num_actions, optimizer)\n",
    "        self.num_actions = num_actions\n",
    "        self.optimizer = optimizer\n",
    "        self.discount = discount\n",
    "        self.dense1 = layers.Dense(64)\n",
    "        self.dense2 = layers.Dense(64)\n",
    "        self.actor  = layers.Dense(self.num_actions)        # Produce logits\n",
    "        self.critic = layers.Dense(1)                      # Produce the state-value directly\n",
    "\n",
    "    def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        x = self.dense2(self.dense1(inputs))\n",
    "        # Then we produce the policy values\n",
    "        action_logits = self.actor(x)                    # 1xnum_actions\n",
    "        action_probs  = tf.nn.softmax(action_logits)     # 1xnum_actions probabilities\n",
    "        # Avoid producing a tensor containing probability 0 for some actions.\n",
    "        action_probs = tf.clip_by_value(action_probs, 1e-10, 1.0)\n",
    "        # ... and the state value.\n",
    "        state_value = self.critic(x)                    # 1x1\n",
    "        return action_logits, action_probs, state_value\n",
    "\n",
    "    def choose_action(self, state:np.ndarray) -> Tuple:\n",
    "        action_logits, action_probs, state_value = self(\n",
    "            tf.expand_dims(tf.cast(state, tf.float32), axis=0))\n",
    "        # Sample from the actions probability distribution\n",
    "        action = tf.random.categorical(action_logits, 1)\n",
    "        return {\n",
    "            'action': action.numpy()[0,0],\n",
    "            'policy': action_probs[0],\n",
    "            'value' : state_value[0]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineActorCriticAgent(2, Adam(learning_rate=1e-2, clipnorm=40.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, state):\n",
    "    a_info = model.choose_action(state)\n",
    "    next_state, reward, done, _ = env.step(a_info['action'])\n",
    "    return (state, a_info, reward, next_state, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(env, init_state):\n",
    "    ep_steps = 0\n",
    "    episode_buffer = deque([], maxlen=200)\n",
    "    state = init_state\n",
    "    for step in tf.range(200):\n",
    "        ep_steps += 1\n",
    "        state, a_info, reward, next_state, done = play_one_step(env, state)\n",
    "        episode_buffer.append((state, a_info, reward, next_state, done))\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    states, actions, rewards, next_states, dones = (\n",
    "        np.array([experience[i] for experience in list(episode_buffer)]) \n",
    "        for i in range(5))\n",
    "    # Compute returns\n",
    "    returns = []\n",
    "    discounted_sum = tf.constant(0.0)\n",
    "    for i in tf.range(ep_steps-1, -1, -1):\n",
    "        discounted_sum = rewards[i] + 0.99 * discounted_sum\n",
    "        returns.append(discounted_sum)\n",
    "    returns = np.stack(returns[::-1])\n",
    "    returns = (returns - tf.math.reduce_mean(returns)) / (tf.math.reduce_std(returns))\n",
    "    return states, actions, rewards, returns, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(env, initial_state):\n",
    "    # We open a GradientTape because we want to reverse these operations to obtain the gradient\n",
    "    # of the loss with respect to the model's parameter\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 1) Get the experience playing the episode\n",
    "        states, actions, rewards, returns, next_states, dones = play_episode(env, initial_state)\n",
    "        v_st_pred = tf.squeeze(tf.stack([a['value'] for a in actions]))\n",
    "        a_probs = tf.stack([a['policy'] for a in actions])\n",
    "        a_indices = tf.stack([a['action'] for a in actions])\n",
    "        a_probs = tf.gather(a_probs, a_indices, batch_dims=1)\n",
    "        a_log_probs = tf.math.log(a_probs)\n",
    "        # Compute delta\n",
    "        delta = returns - v_st_pred\n",
    "        # Actor loss\n",
    "        actor_loss = -tf.reduce_sum(tf.expand_dims(delta, axis=-1)*a_log_probs)\n",
    "        # Critic loss\n",
    "        critic_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)(v_st_pred, returns)\n",
    "        # Entropy loss\n",
    "        # entropy_loss = -tf.reduce_sum(a_log_probs*a_probs)\n",
    "        # Total loss\n",
    "        loss = tf.reduce_sum(actor_loss + critic_loss) #+ 0.01*entropy_loss)\n",
    "    # 9) Obtain the gradient of the loss with respect to the model's parameters\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    # 10) Apply the update\n",
    "    model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode reward: 8.00, mean reward: 16.63: 100%|██████████| 800/800 [01:16<00:00, 10.42it/s]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "rewards_mean = 0\n",
    "with trange(800) as t:\n",
    "    for ep_num in t:\n",
    "        initial_state = env.reset()\n",
    "        rewards = training_step(env, initial_state)\n",
    "        episode_reward = sum(rewards)\n",
    "        rewards_mean = rewards_mean + (episode_reward-rewards_mean)/(ep_num+1)\n",
    "        t.set_description(f'Episode reward: {episode_reward:.2f}, mean reward: {rewards_mean:.2f}')\n",
    "        \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab22f16c00685cd6fecce7c07f73b88874846b6bd1a47397f6c275263024f85c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
