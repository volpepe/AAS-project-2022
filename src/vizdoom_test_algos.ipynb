{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from vizdoom import gym_wrapper\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "from reinforce_agent import BaselineREINFORCEAgent\n",
    "from state import StateManager\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maggioli\\anaconda3\\envs\\rl\\lib\\site-packages\\vizdoom\\gym_wrapper\\base_gym_env.py:51: UserWarning: Detected screen format CRCGCB. Only RGB24 is supported in the Gym wrapper. Forcing RGB24.\n",
      "  warnings.warn(f\"Detected screen format {screen_format.name}. Only RGB24 is supported in the Gym wrapper. Forcing RGB24.\")\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"VizdoomHealthGatheringSupreme-v0\", frame_skip=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can move forward or turn left/right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation space contains:\n",
    "- The 240x320 RGB frame\n",
    "- The health of the player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict(gamevariables:Box(-3.4028235e+38, 3.4028235e+38, (1,), float32), rgb:Box(0, 255, (240, 320, 3), uint8))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline (random moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward: 284.0\n",
      "Episode reward: 380.0\n",
      "Episode reward: 284.0\n",
      "Episode reward: 348.0\n",
      "Episode reward: 380.0\n",
      "Episode reward: 284.0\n",
      "Episode reward: 348.0\n",
      "Episode reward: 380.0\n",
      "Episode reward: 380.0\n",
      "Episode reward: 380.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"VizdoomHealthGatheringSupreme-v0\", frame_skip=4)\n",
    "\n",
    "# Rendering random rollouts for ten episodes\n",
    "rewards = []\n",
    "for _ in range(10):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    ep_rewards = []\n",
    "    while not done:\n",
    "        obs, rew, done, info = env.step(env.action_space.sample())\n",
    "        time.sleep(1/30)\n",
    "        env.render()\n",
    "        ep_rewards.append(rew)\n",
    "    episode_reward = sum(ep_rewards)\n",
    "    print(f\"Episode reward: {episode_reward}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE with Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineREINFORCEAgent(3, Adam(learning_rate=1e-2, clipnorm=40.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, state, state_manager):\n",
    "    a_info = model.choose_action(state, training=True)\n",
    "    next_obv, reward, done, _ = env.step(a_info['action'].value)\n",
    "    next_state = state_manager.get_current_state(next_obv['rgb'].transpose(2,0,1))\n",
    "    return (state, a_info, reward, next_state, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(env, init_state, state_manager):\n",
    "    ep_steps = 0\n",
    "    episode_buffer = deque([], maxlen=2100)\n",
    "    state = init_state\n",
    "    for step in tf.range(2100-1):\n",
    "        ep_steps += 1\n",
    "        state, a_info, reward, next_state, done = play_one_step(env, init_state, state_manager)\n",
    "        episode_buffer.append((state, a_info, reward, next_state, done))\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    states, actions, rewards, next_states, dones = (\n",
    "        np.array([experience[i] for experience in list(episode_buffer)]) \n",
    "        for i in range(5))\n",
    "    # Compute returns\n",
    "    returns = []\n",
    "    discounted_sum = tf.constant(0.0)\n",
    "    for i in tf.range(ep_steps-1, -1, -1):\n",
    "        discounted_sum = rewards[i] + 0.99 * discounted_sum\n",
    "        returns.append(discounted_sum)\n",
    "    returns = np.stack(returns[::-1])\n",
    "    returns = (returns - tf.math.reduce_mean(returns)) / (tf.math.reduce_std(returns))\n",
    "    return states, actions, rewards, returns, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(env, initial_state, state_manager):\n",
    "    # We open a GradientTape because we want to reverse these operations to obtain the gradient\n",
    "    # of the loss with respect to the model's parameter\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 1) Get the experience playing the episode\n",
    "        states, actions, rewards, returns, next_states, dones = play_episode(env, initial_state, state_manager)\n",
    "        v_st_pred = tf.squeeze(tf.stack([a['value'] for a in actions]))\n",
    "        a_probs = tf.stack([a['policy'] for a in actions])\n",
    "        a_indices = tf.stack([a['action'].value for a in actions])\n",
    "        a_probs = tf.gather(a_probs, a_indices, batch_dims=1)\n",
    "        a_log_probs = tf.math.log(a_probs)\n",
    "        # Compute delta\n",
    "        delta = returns - v_st_pred\n",
    "        # Actor loss\n",
    "        actor_loss = -tf.reduce_sum(tf.expand_dims(delta, axis=-1)*a_log_probs)\n",
    "        # Critic loss\n",
    "        critic_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)(\n",
    "            v_st_pred, returns)\n",
    "        # Entropy loss\n",
    "        entropy_loss = -tf.reduce_sum(a_log_probs*a_probs)\n",
    "        # Total loss\n",
    "        loss = tf.reduce_sum(actor_loss + critic_loss) #+ 0.01*entropy_loss)\n",
    "    # 9) Obtain the gradient of the loss with respect to the model's parameters\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    # 10) Apply the update\n",
    "    model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_STEPS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode reward: 284.00, mean reward: 312.80: 100%|██████████| 500/500 [26:43<00:00,  3.21s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "last_scores = deque([], maxlen=10)\n",
    "with trange(TRAINING_STEPS) as t:\n",
    "    for ep_num in t:\n",
    "        state_manager = StateManager()\n",
    "        initial_obs = env.reset()\n",
    "        initial_state = state_manager.get_current_state(initial_obs['rgb'].transpose(2,0,1))\n",
    "        rewards = training_step(env, initial_state, state_manager)\n",
    "        episode_reward = sum(rewards)\n",
    "        last_scores.append(episode_reward)\n",
    "        rewards_mean = np.mean(last_scores)\n",
    "        t.set_description(f'Episode reward: {episode_reward:.2f}, mean reward: {rewards_mean:.2f}')\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some epochs with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_step(env, initial_state, state_manager):\n",
    "    ep_steps = 0\n",
    "    state = initial_state\n",
    "    rewards = []\n",
    "    env.render()\n",
    "    for step in tf.range(2100-1):\n",
    "        ep_steps += 1\n",
    "        a_info = model.choose_action(state)\n",
    "        next_obv, reward, done, _ = env.step(a_info['action'].value)\n",
    "        state = state_manager.get_current_state(next_obv['rgb'].transpose(2,0,1))\n",
    "        env.render()\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maggioli\\anaconda3\\envs\\rl\\lib\\site-packages\\vizdoom\\gym_wrapper\\base_gym_env.py:51: UserWarning: Detected screen format CRCGCB. Only RGB24 is supported in the Gym wrapper. Forcing RGB24.\n",
      "  warnings.warn(f\"Detected screen format {screen_format.name}. Only RGB24 is supported in the Gym wrapper. Forcing RGB24.\")\n",
      "Episode reward: 316.00, mean reward: 332.00: 100%|██████████| 10/10 [00:18<00:00,  1.85s/it]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"VizdoomHealthGatheringSupreme-v0\", frame_skip=4)\n",
    "\n",
    "with trange(10) as t:\n",
    "    for ep_num in t:\n",
    "        state_manager = StateManager()\n",
    "        initial_obs = env.reset()\n",
    "        initial_state = state_manager.get_current_state(initial_obs['rgb'].transpose(2,0,1))\n",
    "        rewards = testing_step(env, initial_state, state_manager)\n",
    "        episode_reward = sum(rewards)\n",
    "        last_scores.append(episode_reward)\n",
    "        rewards_mean = np.mean(last_scores)\n",
    "        t.set_description(f'Episode reward: {episode_reward:.2f}, mean reward: {rewards_mean:.2f}')\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dqn = keras.Sequential([\n",
    "    layers.Input(shape=(42,42,4)),\n",
    "    layers.Conv2D(filters=8, kernel_size=3, strides=(2,2), padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.01)),\n",
    "    layers.Conv2D(filters=16, kernel_size=3, strides=(2,2), padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.01)),\n",
    "    layers.Conv2D(filters=16, kernel_size=3, strides=(2,2), padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.01)),\n",
    "    layers.Conv2D(filters=32, kernel_size=3, strides=(2,2), padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.01)),\n",
    "    layers.Permute((3, 1, 2), input_shape=(3, 3, 32)),\n",
    "    layers.Reshape((32, 9)),\n",
    "    layers.LSTM(64),\n",
    "    keras.layers.Dense(64, activation=\"relu\"),\n",
    "    keras.layers.Dense(32, activation=\"relu\"),\n",
    "    keras.layers.Dense(3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(3)\n",
    "    else:\n",
    "        Q_values = model_dqn.predict(tf.expand_dims(state, axis=0))\n",
    "        return np.argmax(Q_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = deque(maxlen=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
    "    batch = [replay_buffer[index] for index in indices]\n",
    "    states, actions, rewards, next_states, dones = [\n",
    "        np.array([experience[field_index] for experience in batch])\n",
    "        for field_index in range(5)]\n",
    "    return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "discount_factor = 0.99\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss_fn = keras.losses.mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    next_Q_values = model.predict(next_states)\n",
    "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
    "    target_Q_values = (rewards + (1-dones)*discount_factor*max_next_Q_values)\n",
    "    mask = tf.one_hot(actions, 3)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values*mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values,Q_values))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, state, epsilon, state_manager):\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    next_state = state_manager.get_current_state(next_state['rgb'].transpose(2,0,1))\n",
    "    replay_buffer.append((state, action, reward, next_state, done))\n",
    "    return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"VizdoomHealthGatheringSupreme-v0\", frame_skip=4)\n",
    "\n",
    "last_scores = deque([], maxlen=10)\n",
    "with trange(TRAINING_STEPS) as t:\n",
    "    for ep_num in t:\n",
    "        initial_obs = env.reset()\n",
    "        state_manager = StateManager()\n",
    "        initial_state = state_manager.get_current_state(initial_obs['rgb'].transpose(2,0,1))\n",
    "        epsilon = max(1- ep_num/500, 0.01)\n",
    "        state, reward, done, info = play_one_step(env, initial_state, epsilon, state_manager)\n",
    "        env.render()\n",
    "    episode_reward = sum(rewards)\n",
    "    last_scores.append(episode_reward)\n",
    "    rewards_mean = np.mean(last_scores)\n",
    "    t.set_description(f'Episode reward: {episode_reward:.2f}, mean reward: {rewards_mean:.2f}')\n",
    "        \n",
    "env.close()\n",
    "\n",
    "\n",
    "for episode in range(TRAINING_STEPS):\n",
    "    obs = env.reset()\n",
    "    for step in range(200):\n",
    "        epsilon = max(1- episode/500, 0.01)\n",
    "        obs, reward, done, info = play_one_step(env, initial_state, epsilon, state_manager)\n",
    "        env.render()\n",
    "        if done:\n",
    "            break\n",
    "        if episode>50:\n",
    "            training_step(batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('rl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f02a17da643799c7481c294f0deda499520f9707dc713d665cf1db5f41aa1cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
