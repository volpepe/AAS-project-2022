{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from vizdoom import gym_wrapper\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "from agent import Agent\n",
    "from reinforce_agent import BaselineREINFORCEAgent\n",
    "from state import StateManager\n",
    "from action import Action\n",
    "from variables import GAMMA, SIGMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\vizdoom\\gym_wrapper\\base_gym_env.py:51: UserWarning: Detected screen format CRCGCB. Only RGB24 is supported in the Gym wrapper. Forcing RGB24.\n",
      "  warnings.warn(f\"Detected screen format {screen_format.name}. Only RGB24 is supported in the Gym wrapper. Forcing RGB24.\")\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"VizdoomHealthGatheringSupreme-v0\", frame_skip=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can move forward or turn left/right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation space contains:\n",
    "- The 240x320 RGB frame\n",
    "- The health of the player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict(gamevariables:Box(-3.4028235e+38, 3.4028235e+38, (1,), float32), rgb:Box(0, 255, (240, 320, 3), uint8))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineREINFORCEAgent(3, Adam(learning_rate=1e-2, clipnorm=40.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, state, state_manager):\n",
    "    a_info = model.choose_action(state)\n",
    "    next_obv, reward, done, _ = env.step(a_info['action'].value)\n",
    "    next_state = state_manager.get_current_state(next_obv['rgb'].transpose(2,0,1))\n",
    "    return (state, a_info, reward, next_state, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(env, init_state, state_manager):\n",
    "    ep_steps = 0\n",
    "    episode_buffer = deque([], maxlen=2100)\n",
    "    state = init_state\n",
    "    for step in tf.range(2100-1):\n",
    "        ep_steps += 1\n",
    "        state, a_info, reward, next_state, done = play_one_step(env, init_state, state_manager)\n",
    "        episode_buffer.append((state, a_info, reward, next_state, done))\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    states, actions, rewards, next_states, dones = (\n",
    "        np.array([experience[i] for experience in list(episode_buffer)]) \n",
    "        for i in range(5))\n",
    "    # Compute returns\n",
    "    returns = []\n",
    "    discounted_sum = tf.constant(0.0)\n",
    "    for i in tf.range(ep_steps-1, -1, -1):\n",
    "        discounted_sum = rewards[i] + 0.99 * discounted_sum\n",
    "        returns.append(discounted_sum)\n",
    "    returns = np.stack(returns[::-1])\n",
    "    returns = (returns - tf.math.reduce_mean(returns)) / (tf.math.reduce_std(returns))\n",
    "    return states, actions, rewards, returns, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(env, initial_state, state_manager):\n",
    "    # We open a GradientTape because we want to reverse these operations to obtain the gradient\n",
    "    # of the loss with respect to the model's parameter\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 1) Get the experience playing the episode\n",
    "        states, actions, rewards, returns, next_states, dones = play_episode(env, initial_state, state_manager)\n",
    "        v_st_pred = tf.squeeze(tf.stack([a['value'] for a in actions]))\n",
    "        a_probs = tf.stack([a['policy'] for a in actions])\n",
    "        a_indices = tf.stack([a['action'].value for a in actions])\n",
    "        a_probs = tf.gather(a_probs, a_indices, batch_dims=1)\n",
    "        a_log_probs = tf.math.log(a_probs)\n",
    "        # Compute delta\n",
    "        delta = returns - v_st_pred\n",
    "        # Actor loss\n",
    "        actor_loss = -tf.reduce_sum(tf.expand_dims(delta, axis=-1)*a_log_probs)\n",
    "        # Critic loss\n",
    "        critic_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)(\n",
    "            v_st_pred, returns)\n",
    "        # Entropy loss\n",
    "        entropy_loss = -tf.reduce_sum(a_log_probs*a_probs)\n",
    "        # Total loss\n",
    "        loss = tf.reduce_sum(actor_loss + critic_loss) #+ 0.01*entropy_loss)\n",
    "    # 9) Obtain the gradient of the loss with respect to the model's parameters\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    # 10) Apply the update\n",
    "    model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode reward: 348.00, mean reward: 316.00:   2%|▏         | 9/500 [01:22<1:13:51,  9.02s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "last_scores = deque([], maxlen=10)\n",
    "with trange(500) as t:\n",
    "    for ep_num in t:\n",
    "        state_manager = StateManager()\n",
    "        initial_obs = env.reset()\n",
    "        initial_state = state_manager.get_current_state(initial_obs['rgb'].transpose(2,0,1))\n",
    "        rewards = training_step(env, initial_state, state_manager)\n",
    "        episode_reward = sum(rewards)\n",
    "        last_scores.append(episode_reward)\n",
    "        rewards_mean = np.mean(last_scores)\n",
    "        t.set_description(f'Episode reward: {episode_reward:.2f}, mean reward: {rewards_mean:.2f}')\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some epochs with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_step(env, initial_state, state_manager):\n",
    "    ep_steps = 0\n",
    "    state = initial_state\n",
    "    rewards = []\n",
    "    env.render()\n",
    "    for step in tf.range(2100-1):\n",
    "        ep_steps += 1\n",
    "        a_info = model.choose_action(state)\n",
    "        next_obv, reward, done, _ = env.step(a_info['action'].value)\n",
    "        state = state_manager.get_current_state(next_obv['rgb'].transpose(2,0,1))\n",
    "        env.render()\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Volpe\\anaconda3\\envs\\rl\\lib\\site-packages\\vizdoom\\gym_wrapper\\base_gym_env.py:51: UserWarning: Detected screen format CRCGCB. Only RGB24 is supported in the Gym wrapper. Forcing RGB24.\n",
      "  warnings.warn(f\"Detected screen format {screen_format.name}. Only RGB24 is supported in the Gym wrapper. Forcing RGB24.\")\n",
      "Episode reward: 316.00, mean reward: 312.80:  50%|█████     | 5/10 [00:32<00:43,  8.63s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mSi è verificato un arresto anomalo del kernel durante l'esecuzione del codice nella cella attiva o in una cella precedente. Esaminare il codice nelle celle per identificare una possibile causa dell'errore. Per altre informazioni, fare clic su <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a>. Per altri dettagli, vedere Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "env = gym.make(\"VizdoomHealthGatheringSupreme-v0\", frame_skip=4)\n",
    "\n",
    "with trange(10) as t:\n",
    "    for ep_num in t:\n",
    "        state_manager = StateManager()\n",
    "        initial_obs = env.reset()\n",
    "        initial_state = state_manager.get_current_state(initial_obs['rgb'].transpose(2,0,1))\n",
    "        rewards = testing_step(env, initial_state, state_manager)\n",
    "        episode_reward = sum(rewards)\n",
    "        last_scores.append(episode_reward)\n",
    "        rewards_mean = np.mean(last_scores)\n",
    "        t.set_description(f'Episode reward: {episode_reward:.2f}, mean reward: {rewards_mean:.2f}')\n",
    "        \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab22f16c00685cd6fecce7c07f73b88874846b6bd1a47397f6c275263024f85c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
